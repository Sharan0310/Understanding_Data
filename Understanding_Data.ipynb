{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f42cc38",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af32712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from train.txt and test.txt files\n",
    "train_data = pd.read_csv(\"train.txt\", sep=\",\", header=None, names=[\"User\", \"Product\", \"Rating\"])\n",
    "test_data = pd.read_csv(\"test.txt\", sep=\",\", header=None, names=[\"User\", \"Product\", \"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32cbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unwanted in dataset\n",
    "train_data['User'] = train_data['User'].str.extract(r'(\\d+)')\n",
    "train_data['Product'] = train_data['Product'].str.extract(r'(\\d+)')\n",
    "train_data['Rating'] = train_data['Rating'].str.extract(r'(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unwanted in dataset\n",
    "test_data['User'] = test_data['User'].str.extract(r'(\\d+)')\n",
    "test_data['Product'] = test_data['Product'].str.extract(r'(\\d+)')\n",
    "test_data['Rating'] = test_data['Rating'].str.extract(r'(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27774bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a6ac2",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "# Count the total number of unique users and products\n",
    "total_users = train_data['User'].nunique()\n",
    "total_products = train_data['Product'].nunique()\n",
    "\n",
    "print(\"Total number of users:\", total_users)\n",
    "print(\"Total number of products:\", total_products)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448dc709",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48730249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Y matrix by pivot the data\n",
    "Y = train_data.pivot(index='User', columns='Product', values='Rating')\n",
    "\n",
    "# Print the dimensions of the Y matrix\n",
    "print(\"Dimensions of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1169ba2",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cf122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the average rating for each product\n",
    "avg_rating_product = train_data.groupby('Product')['Rating'].mean()\n",
    "# Plotting the histogram\n",
    "plt.hist(avg_rating_product, bins=10,color='purple',edgecolor='black')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Average Ratings of Products')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7cfad2",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97076d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 5 worst products\n",
    "worst_products = avg_rating_product.nsmallest(5)\n",
    "\n",
    "print(\"The 5 worst products:\")\n",
    "print(worst_products)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01b36d",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average rating given by each user\n",
    "avg_rating_user = train_data.groupby('User')['Rating'].mean()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(avg_rating_user, bins=10,color='purple',edgecolor='black')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Average Ratings by Users')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8b718",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f962e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 5 most generous users\n",
    "most_generous_users = avg_rating_user.nlargest(5)\n",
    "\n",
    "print(\"The 5 most generous users:\")\n",
    "print(most_generous_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e73021",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_users_test = test_data['User'].nunique()\n",
    "total_products_test = test_data['Product'].nunique()\n",
    "\n",
    "print(\"Total number of Users in test data:\", total_users_test)\n",
    "print(\"Total number of Products in test data:\", total_products_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357f34a",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the test data to create the X matrix\n",
    "X = test_data.pivot(index='User', columns='Product', values='Rating')\n",
    "\n",
    "# dimensions of the X matrix\n",
    "print(\"Dimensions of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee892937",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa210b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries\n",
    "product_distances = {}\n",
    "top_similar_products = {}\n",
    "\n",
    "# Iterate over each product in the test data\n",
    "for test_product in test_data[\"Product\"].unique():\n",
    "    distances = {}\n",
    "    test_ratings = test_data[test_data[\"Product\"] == test_product].set_index(\"User\")[\"Rating\"]\n",
    "    \n",
    "    # Iterate over each product in the train data\n",
    "    for train_product in train_data[\"Product\"].unique():\n",
    "        train_ratings = train_data[train_data[\"Product\"] == train_product].set_index(\"User\")[\"Rating\"]\n",
    "        \n",
    "        # Compute the distance between the products using the formula\n",
    "        distance = (test_ratings - train_ratings).abs().sum()\n",
    "        distances[train_product] = distance\n",
    "    \n",
    "    # Finding top 5 most similar products \n",
    "    similar_products = sorted(distances, key=distances.get)[:5]\n",
    "    product_distances[test_product] = distances\n",
    "    top_similar_products[test_product] = similar_products\n",
    "\n",
    "# top 5 similar products for each test product\n",
    "print(\"Top 5 similar products:\")\n",
    "for test_product, similar_products in top_similar_products.items():\n",
    "    print(\"Test Product:\", test_product)\n",
    "    print(\"Similar Products:\", similar_products)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930046e2",
   "metadata": {},
   "source": [
    "# 12 Minkowski Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create dictionaries \n",
    "top_similar_products = {}\n",
    "\n",
    "# Iterate over each product in the test data\n",
    "for test_product in test_data[\"Product\"].unique():\n",
    "    distances = {}\n",
    "    test_ratings = test_data[test_data[\"Product\"] == test_product].set_index(\"User\")[\"Rating\"]\n",
    "    \n",
    "    # Iterate over each product in the train data\n",
    "    for train_product in train_data[\"Product\"].unique():\n",
    "        train_ratings = train_data[train_data[\"Product\"] == train_product].set_index(\"User\")[\"Rating\"]\n",
    "        \n",
    "        # Compute the Minkowski Distance between the products with p=2 \n",
    "        distance = np.power(np.abs(test_ratings - train_ratings), 2).sum() ** 0.5\n",
    "        distances[train_product] = distance\n",
    "    \n",
    "    # Find the top 5 most similar products for the current test product\n",
    "    similar_products = sorted(distances, key=distances.get)[:5]\n",
    "    top_similar_products[test_product] = similar_products\n",
    "\n",
    "# the top 5 similar products for each test product\n",
    "print(\"Top 5 similar products:\")\n",
    "for test_product, similar_products in top_similar_products.items():\n",
    "    print(\"Test Product:\", test_product)\n",
    "    print(\"Similar Products:\", similar_products)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ecf52",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df=pd.read_csv('Group 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['customer_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d99223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicates\n",
    "df= df.drop('Unnamed: 0',axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace the values in Class column\n",
    "df['Class'] = df['Class'].replace('Y$e$s$$', 'Churn=Yes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0dede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in Gender, Class column\n",
    "df['gender'] = df['gender'].replace('Female', 0)\n",
    "df['gender'] = df['gender'].replace('Male', 1)\n",
    "df['Class'] = df['Class'].replace('Churn=No', 0)\n",
    "df['Class'] = df['Class'].replace('Churn=Yes', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573328ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the null values in Class\n",
    "df.dropna(subset=['Class'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1685b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the DataType in Class column\n",
    "df['Class'] = df['Class'].astype(int)\n",
    "df['Tenure'] = df['Tenure'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tenure'] = df['Tenure'].apply(lambda x: abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(pd.unique(df['Tenure']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db2b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(pd.unique(df['survey']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace No reply value to Median value\n",
    "df['survey'] = df['survey'].replace('No reply', np.nan)\n",
    "df['survey'] = pd.to_numeric(df['survey'], errors='coerce')\n",
    "\n",
    "median_value = df['survey'].median()\n",
    "df['survey'].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbf2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(pd.unique(df['survey']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f343880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survey']=df['survey'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(pd.unique(df['survey']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6eeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the monthly cost based on the package value\n",
    "df.loc[df['package'] == 1, 'monthly_cost'] = 26\n",
    "df.loc[df['package'] == 2, 'monthly_cost'] = 34\n",
    "df.loc[df['package'] == 3, 'monthly_cost'] = 40\n",
    "df.loc[df['package'] == 4, 'monthly_cost'] = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74483f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding Total_Cost\n",
    "df['Total_cost']=df['monthly_cost']*df['Tenure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "known_dependents = df[df['dependents'] != 'Unknown']\n",
    "unknown_dependents = df[df['dependents'] == 'Unknown']\n",
    "\n",
    "# Exclude 'customer_id' and 'dependents' columns\n",
    "X = known_dependents.drop(['dependents', 'customer_id'], axis=1)\n",
    "y = known_dependents['dependents']\n",
    "\n",
    "# Perform one-hot encoding on categorical columns\n",
    "categorical_cols = ['gender', 'location', 'partner', 'senior', 'package', 'survey', 'Class']\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_cols]))\n",
    "X_encoded.columns = encoder.get_feature_names(categorical_cols)\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X.drop(categorical_cols, axis=1)))\n",
    "X_imputed.columns = X.drop(categorical_cols, axis=1).columns\n",
    "\n",
    "# Concatenate the encoded features and imputed values\n",
    "X_final = pd.concat([X_imputed, X_encoded], axis=1)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_final, y)\n",
    "\n",
    "# Preprocess the unknown data \n",
    "unknown_X = unknown_dependents.drop(['dependents', 'customer_id'], axis=1)\n",
    "unknown_X_encoded = pd.DataFrame(encoder.transform(unknown_X[categorical_cols]))\n",
    "unknown_X_encoded.columns = encoder.get_feature_names(categorical_cols)\n",
    "unknown_X_imputed = pd.DataFrame(imputer.transform(unknown_X.drop(categorical_cols, axis=1)))\n",
    "unknown_X_imputed.columns = unknown_X.drop(categorical_cols, axis=1).columns\n",
    "unknown_X_final = pd.concat([unknown_X_imputed, unknown_X_encoded], axis=1)\n",
    "\n",
    "unknown_dependents['dependents'] = model.predict(unknown_X_final)\n",
    "\n",
    "df[df['dependents'] == 'Unknown'] = unknown_dependents\n",
    "\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dependents'] = df['dependents'].replace('Unknown', pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69d815",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(pd.unique(df['dependents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dependents'] = df['dependents'].astype(int)\n",
    "df['Total_cost'] = df['Total_cost'].astype(int)\n",
    "df['monthly_cost'] = df['monthly_cost'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60574b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(pd.unique(df['monthly_cost']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27032b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'location' column\n",
    "df['location'] = label_encoder.fit_transform(df['location'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ad76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# first few rows\n",
    "print(df.head())\n",
    "\n",
    "# summary statistics \n",
    "print(df.describe())\n",
    "\n",
    "# data types of each column \n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5930ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f4aedf",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Selecting four columns for correlation\n",
    "selected_columns = ['Tenure', 'monthly_cost', 'partner', 'dependents']\n",
    "selected_df = df[selected_columns]\n",
    "\n",
    "# for the correlation heatmap\n",
    "sns.heatmap(selected_df.corr(), annot=True, cmap='summer')\n",
    "plt.title(\"Correlation Heatmap for Selected Columns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f60bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable 'Class'\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Distribution of Churn (Class)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the distribution of numerical features\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "# for the subplots Calculating the number of rows and columns\n",
    "num_rows = (len(num_cols) - 1) // 3 + 1\n",
    "num_cols_subplot = min(3, len(num_cols))\n",
    "\n",
    "# Create grid of subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols_subplot, figsize=(15, 5*num_rows))\n",
    "\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    row = i // num_cols_subplot\n",
    "    col_subplot = i % num_cols_subplot\n",
    "    sns.histplot(df[col], kde=True, ax=axs[row, col_subplot], color='purple')  # Set the color to purple\n",
    "    axs[row, col_subplot].set_title(f\"Distribution of {col}\")\n",
    "    axs[row, col_subplot].set_xlabel(col)\n",
    "\n",
    "\n",
    "for i in range(len(num_cols), num_rows * num_cols_subplot):\n",
    "    axs.flat[i].set_visible(False)\n",
    "\n",
    "# tight the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfcc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create grid of subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# custom color palettes \n",
    "colors1 = ['purple', 'yellow', 'green','orange']\n",
    "colors2 = ['orange', 'purple']\n",
    "colors3 = ['gold', 'dodgerblue']\n",
    "\n",
    "# Plot the churn rate by package\n",
    "sns.barplot(x='package', y='Class', data=df, ax=axs[0], palette=colors1)\n",
    "axs[0].set_title('Churn Rate by Package')\n",
    "axs[0].set_xlabel('Package')\n",
    "axs[0].set_ylabel('Churn Rate')\n",
    "\n",
    "# Plot the churn rate by gender and seniority\n",
    "sns.barplot(x='gender', y='Class', hue='senior', data=df, ax=axs[1], palette=colors2)\n",
    "axs[1].set_title('Churn Rate by Gender and Seniority')\n",
    "axs[1].set_xlabel('Gender')\n",
    "axs[1].set_ylabel('Churn Rate')\n",
    "\n",
    "# Box plot of Monthly Cost by Gender and Churn\n",
    "sns.boxplot(x='gender', y='monthly_cost', hue='Class', data=df, ax=axs[2], palette=colors2)\n",
    "axs[2].set_title('Monthly Cost Distribution by Gender and Churn')\n",
    "axs[2].set_xlabel('Gender')\n",
    "axs[2].set_ylabel('Monthly Cost')\n",
    "\n",
    "# layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2945f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Create grid of subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "custom_palette = ['lightblue', 'purple']\n",
    "\n",
    "# distribution of the gender feature\n",
    "sns.countplot(x='gender', hue='Class', data=df, ax=axs[0, 0], palette=custom_palette)\n",
    "axs[0, 0].set_title('Distribution of Churn by Gender')\n",
    "\n",
    "# distribution of the location feature\n",
    "sns.countplot(x='location', hue='Class', data=df, ax=axs[0, 1], palette=custom_palette)\n",
    "axs[0, 1].set_title('Distribution of Churn by Location')\n",
    "\n",
    "# distribution of the partner feature\n",
    "sns.countplot(x='partner', hue='Class', data=df, ax=axs[0, 2], palette=custom_palette)\n",
    "axs[0, 2].set_title('Distribution of Churn by Partner')\n",
    "\n",
    "# distribution of the dependents feature\n",
    "sns.countplot(x='dependents', hue='Class', data=df, ax=axs[1, 0], palette=custom_palette)\n",
    "axs[1, 0].set_title('Distribution of Churn by Dependents')\n",
    "\n",
    "# distribution of the senior feature\n",
    "sns.countplot(x='senior', hue='Class', data=df, ax=axs[1, 1], palette=custom_palette)\n",
    "axs[1, 1].set_title('Distribution of Churn by Seniority')\n",
    "\n",
    "# Hide the empty subplot\n",
    "axs[1, 2].axis('off')\n",
    "\n",
    "# the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a343dd",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Prepare features (X) and target variable (y)\n",
    "X = df.drop(['customer_id', 'Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "\n",
    "# feature importance scores in descending order\n",
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importance, y=feature_importance.index, palette='winter')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb89ab",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba830e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Prepare features (X) and target variable (y)\n",
    "X = df.drop(['customer_id', 'Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features for models \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "logistic_regression_model = LogisticRegression()\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "gradient_boosting_model = GradientBoostingClassifier(random_state=42)\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Train the models\n",
    "logistic_regression_model.fit(X_train_scaled, y_train)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "gradient_boosting_model.fit(X_train, y_train)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = logistic_regression_model.predict(X_test_scaled)\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "y_pred_gb = gradient_boosting_model.predict(X_test)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nRandom Forest Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\nGradient Boosting Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\n",
    "print(\"\\nSupport Vector Machine (SVM) Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9892d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store the model names and their corresponding accuracy\n",
    "models_accuracy = {\n",
    "    'Logistic Regression': accuracy_score(y_test, y_pred_lr),\n",
    "    'Random Forest Classifier': accuracy_score(y_test, y_pred_rf),\n",
    "    'Gradient Boosting Classifier': accuracy_score(y_test, y_pred_gb),\n",
    "    'Support Vector Machine (SVM) Classifier': accuracy_score(y_test, y_pred_svm)\n",
    "}\n",
    "\n",
    "# Find the model with the highest accuracy\n",
    "best_model = max(models_accuracy, key=models_accuracy.get)\n",
    "best_accuracy = models_accuracy[best_model]\n",
    "\n",
    "# Print the best model and its accuracy\n",
    "print(\"Best Model:\")\n",
    "print(best_model)\n",
    "print(\"Accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "# Convert 'location' column to integers using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['location'] = label_encoder.fit_transform(df['location'])\n",
    "\n",
    "# Prepare features (X) and target variable (y)\n",
    "X = df.drop(['customer_id', 'Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d19337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the churn rate by survey response\n",
    "sns.barplot(x='survey', y='Class', data=df)\n",
    "plt.title('Churn Rate by Survey Response')\n",
    "plt.xlabel('Survey Response')\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the churn rate by seniority and package\n",
    "sns.barplot(x='senior', y='Class', hue='package', data=df)\n",
    "plt.title('Churn Rate by Seniority and Package')\n",
    "plt.xlabel('Seniority')\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751afefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create grid of subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# custom color palettes \n",
    "colors1 = ['purple', 'yellow', 'green','orange']\n",
    "colors2 = ['orange', 'purple']\n",
    "colors3 = ['gold', 'dodgerblue']\n",
    "\n",
    "# Plot the churn rate by package\n",
    "sns.barplot(x='package', y='Class', data=df, ax=axs[0], palette=colors1)\n",
    "axs[0].set_title('Churn Rate by Package')\n",
    "axs[0].set_xlabel('Package')\n",
    "axs[0].set_ylabel('Churn Rate')\n",
    "\n",
    "# Plot the churn rate by gender and seniority\n",
    "sns.barplot(x='gender', y='Class', hue='senior', data=df, ax=axs[1], palette=colors2)\n",
    "axs[1].set_title('Churn Rate by Gender and Seniority')\n",
    "axs[1].set_xlabel('Gender')\n",
    "axs[1].set_ylabel('Churn Rate')\n",
    "\n",
    "# Box plot of Monthly Cost by Gender and Churn\n",
    "sns.boxplot(x='gender', y='monthly_cost', hue='Class', data=df, ax=axs[2], palette=colors2)\n",
    "axs[2].set_title('Monthly Cost Distribution by Gender and Churn')\n",
    "axs[2].set_xlabel('Gender')\n",
    "axs[2].set_ylabel('Monthly Cost')\n",
    "\n",
    "# layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
